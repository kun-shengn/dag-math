import os # å¯¼å…¥ os åº“ï¼Œç”¨äºä¸æ“ä½œç³»ç»Ÿäº¤äº’ï¼Œä¾‹å¦‚è®¾ç½®ç¯å¢ƒå˜é‡
import re # å¯¼å…¥ re åº“ï¼Œç”¨äºæ­£åˆ™è¡¨è¾¾å¼æ“ä½œï¼Œä¾‹å¦‚æå–ç­”æ¡ˆ
import torch # å¯¼å…¥ PyTorch åº“
import logging

from abc import ABC, abstractmethod
from config.log_config import setup_logger
from src.dataset.process import DeepMath_processer
from config.rl_llm_config import Qwen3_4B_GRPO_Config_DeepMath, set_grpo_config
from datasets import load_dataset, Dataset # å¯¼å…¥ datasets åº“ï¼Œç”¨äºåŠ è½½å’Œå¤„ç†æ•°æ®é›†
from transformers import AutoTokenizer, AutoModelForCausalLM # å¯¼å…¥ transformers åº“ï¼Œç”¨äºåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œ tokenizer
from trl.trainer import GRPOConfig, GRPOTrainer # å¯¼å…¥ trl åº“ä¸­çš„ GRPOConfig å’Œ GRPOTrainerï¼Œç”¨äº GRPO è®­ç»ƒ

# é…ç½® logger
logger = setup_logger(__name__)
# # ğŸŒŸ æ ¸å¿ƒä¿®å¤ï¼šå¼ºåˆ¶å½“å‰è¿›ç¨‹åªä½¿ç”¨è‡ªå·±åˆ†é…åˆ°çš„é‚£å¼ ç‰©ç†å¡ï¼Œæœç»äº¤å‰å ç”¨ï¼
local_rank = int(os.environ.get("LOCAL_RANK", 0))
torch.cuda.set_device(local_rank)

# ---------------------------------------------------------
# å®šä¹‰å¤§æ¨¡å‹GRPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¥å£
# ---------------------------------------------------------
class LLM_GRPO(ABC):
    """å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¥å£"""
    @abstractmethod
    def train(self, model_dir: str):
        pass

class Qwen3_4B_LLM_GRPO(LLM_GRPO):
    """åŸºäº Qwen3-4B æ¨¡å‹çš„ GRPO å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®ç°"""
    def __init__(self):
        pass

        # ä» XML æ ¼å¼æ–‡æœ¬ä¸­æå–ç­”æ¡ˆï¼Œä¾‹å¦‚ä» <answer>42</answer> ä¸­æå– "42"
    def extract_xml_answer(self, text: str) -> str:
        try:
            answer = text.split("<answer>")[-1].split("</answer>")[0].strip() # ä½¿ç”¨ <answer> å’Œ </answer> åˆ†å‰²æ–‡æœ¬ï¼Œæå–ç­”æ¡ˆå¹¶å»é™¤é¦–å°¾ç©ºæ ¼
            return answer
        except IndexError:
            return "" # å¦‚æœæå–å¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        
        # å®šä¹‰å¥–åŠ±å‡½æ•° (reward functions)

    # æ ¼å¼å¥–åŠ±å‡½æ•°ï¼šæ£€æŸ¥æ¨¡å‹è¾“å‡ºæ˜¯å¦ç¬¦åˆ XML æ ¼å¼è¦æ±‚ (åŒ…å« <reasoning> å’Œ <answer> æ ‡ç­¾)
    def format_reward_func(self, completions, **kwargs) -> list[float]:
        """Reward function that checks if the completion has the correct format."""
        pattern = r"^<reasoning>.*?</reasoning>\s*<answer>.*?</answer>$" # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é…ä»¥ <reasoning> å¼€å¤´ï¼Œ</reasoning> ç»“å°¾ï¼Œä¸­é—´ä»»æ„å­—ç¬¦ï¼Œç„¶åæ˜¯ <answer> å¼€å¤´ï¼Œ</answer> ç»“å°¾ï¼Œä¸­é—´ä»»æ„å­—ç¬¦çš„æ ¼å¼
        responses = [completion[0]["content"] for completion in completions] # ä» completions ä¸­æå–æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
        matches = [bool(re.match(pattern, r)) for r in responses] # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦ç¬¦åˆæ ¼å¼
        return [1.0 if match else 0.0 for match in matches] # å¦‚æœåŒ¹é…ï¼Œå¥–åŠ± 1.0ï¼Œå¦åˆ™å¥–åŠ± 0.0

    # æ­£ç¡®æ€§å¥–åŠ±å‡½æ•°ï¼šæ£€æŸ¥æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦ä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´
    def correctness_reward_func(self, prompts, completions, answer, **kwargs) -> list[float]:
        """Reward function that checks if the answer is correct."""
        responses = [completion[0]['content'] for completion in completions] # æå–æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
        extracted_responses = [self.extract_xml_answer(r) for r in responses] # ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå– XML æ ¼å¼çš„ç­”æ¡ˆ
        print(f"Question: {prompts[0][-1]['content']}\nAnswer: {answer[0]}\nResponse: {responses[0]}\nExtracted: {extracted_responses[0]}") # æ‰“å°é—®é¢˜ã€æ ‡å‡†ç­”æ¡ˆã€æ¨¡å‹å®Œæ•´è¾“å‡ºå’Œæå–å‡ºçš„ç­”æ¡ˆï¼Œç”¨äºè°ƒè¯•
        print(''.join('âœ…' if r == a else 'âŒ' for r, a in zip(extracted_responses, answer))) # æ‰“å° âœ… æˆ– âŒï¼Œè¡¨ç¤ºç­”æ¡ˆæ˜¯å¦æ­£ç¡®
        return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)] # å¦‚æœç­”æ¡ˆæ­£ç¡®ï¼Œå¥–åŠ± 2.0ï¼Œå¦åˆ™å¥–åŠ± 0.0

    def train(self, model_dir: str, dataset_dir: str = r"D:\code\mess\datasets\deepmath-103k"):
        # è®­ç»ƒä»£ç ç¤ºä¾‹
        logger.info("Starting GRPO training for Qwen3-4B model...")
        processer = DeepMath_processer()
        dataset = processer.process_dataset(dataset_dir = dataset_dir)
        # è®¾ç½®æ˜¾å­˜ç›¸å…³çš„ç¯å¢ƒå˜é‡ï¼Œé™åˆ¶ PyTorch æ˜¾å­˜åˆ†é…ç­–ç•¥ï¼Œé˜²æ­¢ OOM
        # os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
        training_args = set_grpo_config(Qwen3_4B_GRPO_Config_DeepMath()) # åˆå§‹åŒ– GRPO è®­ç»ƒå‚æ•°é…ç½®

        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_dir, 
            torch_dtype=torch.bfloat16, 
            # device_map={"": local_rank}, # æ ¸å¿ƒä¿®å¤ï¼šå¼ºåˆ¶å½“å‰è¿›ç¨‹çš„æ¨¡å‹ä¸¥æ ¼åŠ è½½åˆ°å¯¹åº”çš„ GPU ä¸Š
        )

        # åŠ è½½ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_dir, # tokenizer åç§°
            model_max_length=training_args.max_completion_length, # è®¾ç½® tokenizer æœ€å¤§é•¿åº¦
        )
        tokenizer.pad_token = tokenizer.eos_token # å°† pad token è®¾ç½®ä¸º eos token

        # åˆå§‹åŒ– GRPO Trainer
        trainer = GRPOTrainer(
            model=model, # ä¼ å…¥æ¨¡å‹
            processing_class=tokenizer, # ä¼ å…¥ tokenizer
            reward_funcs=[ # ä¼ å…¥å¥–åŠ±å‡½æ•°åˆ—è¡¨
                self.format_reward_func, # æ ¼å¼å¥–åŠ±å‡½æ•°
                self.correctness_reward_func # æ­£ç¡®æ€§å¥–åŠ±å‡½æ•°
            ],
            args=training_args, # ä¼ å…¥è®­ç»ƒå‚æ•°
            train_dataset=dataset['train'], # ä¼ å…¥è®­ç»ƒæ•°æ®é›†
        )

        # å¼€å§‹è®­ç»ƒ
        trainer.train()

# ---------------------------------------------------------
if __name__ == "__main__":
    grpo_trainer = Qwen3_4B_LLM_GRPO()
    grpo_trainer.train(model_dir = "/mnt/iso/zqs_workspace/model/Qwen3-4B", dataset_dir = "/mnt/iso/zqs_workspace/dataset/deepmath-103k")
